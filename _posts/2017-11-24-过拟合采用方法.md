---
layout:     post
title:      机器学习过拟合常见采用方法
subtitle:   机器学习 过拟合
date:       2017-11-24
author:     zk
catalog: true
tags:
    - 机器学习
    - 过拟合
--- 
# 机器学习过拟合常见采用方法

在对模型进行训练时，有可能遇到训练数据不够，或者模型参数的问题。常常会导致模型的过拟合（overfitting），即模型复杂度比实际数据复杂度还要高。  
通过数据的流程可以分别从数据源头，模型参数和惩罚和多分类等方面实现对过拟合的控制。
## 数据集
 1.信号去噪（数据清洗）  
 数据清洗是指发现并纠正数据文件中可识别的错误的最后一道程序，包括检查数据一致性，处理无效值和缺失值等。与问卷审核不同，录入后的数据清理一般是由计算机而不是人工完成。可以参考知乎（https://www.zhihu.com/question/22077960）  
 2.增加训练数据集  
 收集和构造新数据：对实验可采集能涉及到的各方面数据，只有当训练数据涉及各方面才能实现更好的分类。  
 常见采用方法：  
 * 源头取数
 * 复制数据+随机噪声
 * 重采样
 * 分布估计  


3.验证数据  
验证最小误差的迭代次数，常用交叉验证。
## 模型参数

1. 选择合适的迭代停止条件  
1. 迭代过程中进行权值衰减（以某个小因子降低每个权值）  
3. 减少特征数目
    不相关的特征会对分类器产生影响，因此特征子集筛选是特征工程的重要步骤，关于特征筛选的一些算法介绍后面专门总结。  
4.对于决策树可以采用剪枝法  
    决策树剪枝是剪除掉冗余和重复的枝条。

## 增加惩罚
1.正则化（L1,L2）
    贴一个视频链接：https://www.youtube.com/watch?v=TmzzQoO8mr4   

* L1正则  
L1正则是基于L1范数，即在目标函数后面加上参数的L1范数和项，即参数绝对值和与参数的积项。  
    其中C0代表原始的代价函数，n是样本的个数，λ就是正则项系数，权衡正则项与C0项的比重。后面那一项即为L1正则项。
* L2正则  
L2正则是基于L2范数，即在目标函数后面加上参数的L2范数和项，即参数的平方和与参数的积项。  

2.Dropout（修改神经网络本身结构）  
神经网络中，有一种方法是通过修改神经网络本身结构来实现的，其名为Dropout。该方法是在对网络进行训练时用一种技巧（trike）。参见博客：http://blog.csdn.net/ztf312/article/details/51084950
## 采用组合分类器（装袋或者随机森林）
随机森林和Bagging 算法是通过多分类器投票实现的，可以很好的防止过拟合。其他分类器的组合也属于组合分类器。  
# 参考
http://blog.csdn.net/ztf312/article/details/51084950
https://liam0205.me/2017/03/30/L1-and-L2-regularizer/
http://www.cnblogs.com/xiangzhi/p/4639464.html

